{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import CXRBERT\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoModel, AutoModelForSequenceClassification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADGRAPH_CLASSES = ['O', 'B-ANAT-DP', 'I-ANAT-DP', 'B-OBS-DP', 'I-OBS-DP', 'B-OBS-DA', 'I-OBS-DA', 'B-OBS-U', 'I-OBS-U']\n",
    "unique_labels = RADGRAPH_CLASSES\n",
    "labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n",
    "ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n",
    "print(labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('/vol/space/users/zengin/ner/train_ner_small.csv', converters={'sentences': literal_eval, 'labels': literal_eval,  'ner_tags': literal_eval})\n",
    "val_new_df = pd.read_csv('/vol/space/users/zengin/ner/val_ner.csv', converters={'sentences': literal_eval, 'labels': literal_eval,  'ner_tags': literal_eval})\n",
    "test_new_df = pd.read_csv('/vol/space/users/zengin/ner/test_ner.csv', converters={'sentences': literal_eval, 'labels': literal_eval,  'ner_tags': literal_eval})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tds = Dataset.from_pandas(new_df)\n",
    "val_tds = Dataset.from_pandas(val_new_df)\n",
    "test_tds = Dataset.from_pandas(test_new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code = True)\n",
    "#model = AutoModel.from_pretrained(\"microsoft/BiomedVLP-CXR-BERT-specialized\", trust_remote_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a sanity check for the tokens\n",
    "inputs = tokenizer(tds[1][\"sentences\"], is_split_into_words=True)\n",
    "print(inputs.tokens())\n",
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    new_tags = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label) \n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            \n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentences\"], truncation = True, is_split_into_words=True)\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenize_and_align_labels(tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_datasets = tds.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=tds.column_names,\n",
    ")\n",
    "val_tokenized_datasets = val_tds.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=val_tds.column_names,\n",
    ")\n",
    "\n",
    "test_tokenized_datasets = test_tds.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=test_tds.column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_metrics(results_agg):\n",
    "    results = {}\n",
    "    for key in results_agg.keys():\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "        f1 = 0\n",
    "        for key2 in results_agg[key].keys():\n",
    "            prec += results_agg[key][key2]['precision']\n",
    "            rec += results_agg[key][key2]['recall']\n",
    "            f1 += results_agg[key][key2]['f1']\n",
    "        prec = prec/4\n",
    "        rec = rec/4\n",
    "        f1 = f1/4\n",
    "        results[key] = {'precision': prec, 'recall': rec, 'f1': f1}\n",
    "    return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "from seqeval.metrics import classification_report\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[ids_to_labels[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [ids_to_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    \n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels, mode = 'strict', scheme = 'IOB2')\n",
    "    report = classification_report(true_predictions, true_labels)\n",
    "    evaluator = Evaluator(true_labels, true_predictions, tags= ['ANAT-DP', 'OBS-DP', 'OBS-DA', 'OBS-U'], loader=\"list\")\n",
    "    #global results\n",
    "    #global results_agg\n",
    "    results, results_agg = evaluator.evaluate()\n",
    "    results_dict = average_metrics(results_agg)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "        'report': report,\n",
    "        'muc_results':results_dict\n",
    "        \n",
    "        #'nereval_results': results,\n",
    "        #'nereval_results_agg': results_agg\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the pre-trained models\n",
    "# path = './CXR-BERT/2v0m7nnw/checkpoints/epoch=49-step=25000.ckpt'\n",
    "# model_cpt = torch.load(path)\n",
    "\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in model_cpt['state_dict'].items():\n",
    "#     # if 'bert' in k:\n",
    "#     #     name = k[11:] # remove `module.`\n",
    "#     #     new_state_dict[name] = v\n",
    "#     # else:\n",
    "#     name = k[6:] # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "\n",
    "#model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model2 = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=ids_to_labels,\n",
    "    label2id=labels_to_ids,\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "#for pre-trained models in this thesis\n",
    "# model2.bert.embeddings = model.base_model.embeddings\n",
    "# model2.bert.encoder = model.base_model.encoder\n",
    "\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ##freezing params for main model and only training the classifier layers\n",
    "# for param in model2.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "# ###doesnt work it underfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one forward pass to check if the code is working correctly\n",
    "outputs = model2(input_ids=torch.tensor(train_tokenized_datasets['input_ids'][0]).unsqueeze(0).to(device), labels = torch.tensor(train_tokenized_datasets[0]['labels']).unsqueeze(0).to(device))\n",
    "outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "metric_name = \"precision\"\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    seed = 123,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=args,\n",
    "    train_dataset=train_tokenized_datasets,\n",
    "    eval_dataset=val_tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(test_tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
