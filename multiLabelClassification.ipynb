{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Tuple, Union\n",
    "import functools\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor as T\n",
    "from transformers import BertForMaskedLM, BertConfig, BertTokenizer, AutoModel\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import CXRBERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'allenai/scibert_scivocab_cased'\n",
    "config = CXRBERT.CXRBertConfig.from_pretrained(checkpoint)\n",
    "tokenizer = CXRBERT.CXRBertTokenizer.from_pretrained(checkpoint,padding=\"max_length\", truncation=True, max_length=512)\n",
    "model = CXRBERT.CXRBertModel(config).from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './CXR-BERT/2v87h10f/checkpoints/epoch=49-step=25000.ckpt'\n",
    "\n",
    "\n",
    "model_cpt = torch.load(path)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in model_cpt['state_dict'].items():\n",
    "    name = k[6:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"5000_train_data_full_reports.csv\",\n",
    "                                           \"validation\": \"full_val_with_c.csv\",\n",
    "                                             \"test\": \"full_test_with_c.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['subject_id', 'study_id', 'report','c_report']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, do_lower_case = False)\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"c_report\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "encoded_dataset.set_format(\"torch\", device = device)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id,\n",
    "                                                          #attention_probs_dropout_prob=0.5,\n",
    "                                                          #hidden_dropout_prob=0.5,\n",
    "                                                           )\n",
    "\n",
    "#for pre-trained models in this thesis we swap the weights with the model architecture\n",
    "# model2.bert.embeddings = model.base_model.embeddings\n",
    "# model2.bert.encoder = model.base_model.encoder\n",
    "##model2.bert.pooler.dense = torch.nn.Linear(in_features=768, out_features=768, bias=True)\n",
    "## model2.bert.pooler.activation = torch.nn.Tanh()\n",
    "\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##freezing params for main model and only training the classifier layers for experiments\n",
    "# for param in model2.bert.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "metric_name = \"f1_macro\"\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"./\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate= 3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= 4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #save_total_limit=1,\n",
    "    #push_to_hub=True,\n",
    "    seed = 9000,\n",
    "    report_to = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_recall_fscore_support, roc_curve\n",
    "from transformers import EvalPrediction\n",
    "import evaluate\n",
    "\n",
    "def my_multi_label_metrics(predictions, labels):\n",
    "    # Convert the predictions and labels to numpy arrays\n",
    "    #preds = predictions #.detach().cpu().numpy()\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    preds = sigmoid(torch.Tensor(predictions))\n",
    "    labels = labels #.detach().cpu().numpy()\n",
    "    # Initialize the thresholds, precision, and recall arrays\n",
    "    thresholds = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        precision, recall, threshold = precision_recall_curve(labels[:, i], preds[:, i])\n",
    "        thresholds.append(threshold)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    f1_scores = 2 * (np.array(precisions) * np.array(recalls)) / (np.array(precisions) + np.array(recalls))\n",
    "    best_thresholds = [thresholds[i][np.argmax(f1_scores[i])] for i in range(len(f1_scores))]\n",
    "    print(best_thresholds)\n",
    "\n",
    "\n",
    "    \n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    #y_pred = np.zeros(probs.shape)\n",
    "    #print(probs.shape)\n",
    "    pred_labels = np.zeros_like(probs)\n",
    "    for i in range(preds.shape[1]):\n",
    "        pred_labels[:, i] = np.where(preds[:, i] > best_thresholds[i], 1, 0)\n",
    "    \n",
    "    y_pred = pred_labels\n",
    "    y_true = labels\n",
    "\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, probs, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "    #roc_auc_mac = roc_auc_score(y_true, y_pred, average = 'macro')\n",
    "    roc_auc_mac = roc_auc_score(y_true, probs, average = 'macro')\n",
    "    f1_w_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    roc_auc_w = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "    #############################################################################\n",
    "    roc_auc_score2 = evaluate.load(\"roc_auc\", \"multilabel\")\n",
    "    results = roc_auc_score2.compute(references=y_true, prediction_scores=probs, average = None)['roc_auc']\n",
    "    \n",
    "    #precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    # return as dictionary\n",
    "    metrics = {'f1_micro': f1_micro_average,\n",
    "               'roc_auc_micro': roc_auc,\n",
    "               'f1_macro': f1_macro_average,\n",
    "               'roc_auc_macro': roc_auc_mac,\n",
    "               'f1_weighted': f1_w_average,\n",
    "               'roc_auc_weighted': roc_auc_w,\n",
    "               'accuracy': accuracy,\n",
    "               'roc_auc_per_class': [round(res, 3) for res in results]\n",
    "               }\n",
    "    return metrics\n",
    "\n",
    "def my_compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = my_multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model2,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"], \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=my_compute_metrics,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pred = trainer.predict(encoded_dataset[\"test\"])\n",
    "raw_pred.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
