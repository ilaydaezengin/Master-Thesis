{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from datasets import ClassLabel\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel, AutoTokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "import CXRBERT\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"allenai/scibert_scivocab_cased\"\n",
    "config = CXRBERT.CXRBertConfig.from_pretrained(checkpoint)\n",
    "tokenizer = CXRBERT.CXRBertTokenizer.from_pretrained(checkpoint,padding=\"max_length\", truncation=True, max_length=128)\n",
    "model = CXRBERT.CXRBertModel(config).from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the pre-trained models\n",
    "# path = './CXR-BERT/2v0m7nnw/checkpoints/epoch=49-step=25000.ckpt'\n",
    "# model_cpt = torch.load(path)\n",
    "\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in model_cpt['state_dict'].items():\n",
    "#     name = k[6:] # remove `module.`\n",
    "#     new_state_dict[name] = v\n",
    "\n",
    "\n",
    "# model.load_state_dict(new_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "g_labels = ClassLabel(names = ['contradiction', 'entailment', 'neutral'])\n",
    "int2label = {0: \"contradiction\", 1: \"entailment\", 2: \"neutral\"}\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"mednli_train.csv\", \n",
    "                                                    'validation': \"radnli_val.csv\", \n",
    "                                                    'test': \"radnli_test.csv\"})\n",
    "\n",
    "x =  [label2int[element] for element in dataset['train']['gold_label']]\n",
    "y = [label2int[element] for element in dataset['validation']['gold_label']]\n",
    "z = [label2int[element] for element in dataset['test']['gold_label']]\n",
    "\n",
    "dataset['train']=dataset['train'].add_column('label',x)\n",
    "dataset['validation']=dataset['validation'].add_column('label',y)\n",
    "dataset['test']=dataset['test'].add_column('label',z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.rename_column (\"sentence1\", \"premise\")\n",
    "dataset = dataset.rename_column (\"sentence2\", \"hypothesis\")\n",
    "dataset = dataset.remove_columns (['gold_label', 'pair_id'])#, 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to visualise the dataset\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "labels = g_labels.names\n",
    "try:\n",
    "# Graph for split wise graph\n",
    "  split_wise_labels = [key for key, value in dataset.items ()]\n",
    "  split_wise_numbers = [value.num_rows for key, value in dataset.items ()]\n",
    "  split_wise_explode = [random.randint (1, 5) / 10 if key != 'train' else 0 for key, value in dataset.items ()]\n",
    "\n",
    "  sen_length = {}\n",
    "\n",
    "  # Graph for total label wise split\n",
    "  split_wise_label_wise = {}\n",
    "  total_label_wise_split = [0] * len (labels)\n",
    "  for key, value in dataset.items ():\n",
    "    counter = collections.Counter (value['label'])\n",
    "    temp = [0] * len (labels)\n",
    "    for index, total in counter.items ():\n",
    "      temp[ index ] += total\n",
    "    split_wise_label_wise[key] = temp  \n",
    "    total_label_wise_split = [x + y for x, y in zip(total_label_wise_split, temp)]\n",
    "\n",
    "    sen_length[key] = {'premise': [], 'hypothesis': [], 'sum': []}\n",
    "    \n",
    "    for element in value:\n",
    "      sen_length[key]['premise'].append ( len(element['premise'].split ()) )\n",
    "      sen_length[key]['hypothesis'].append ( len(element['hypothesis'].split ()) )\n",
    "      sen_length[key]['sum'].append (sen_length[key]['premise'][-1] + sen_length[key]['hypothesis'][-1])\n",
    "    \n",
    "  # Graph for split wise and label wise\n",
    "  split_wise_label_wise = [split_wise_label_wise[label] for label in split_wise_labels]\n",
    "  split_wise_label_wise = [[split_wise_label_wise[j][i] for j in range(len(split_wise_label_wise))] for i in range(len(split_wise_label_wise[0]))]\n",
    "\n",
    "  fig, axs = plt.subplots(2,3, figsize=(20,10))\n",
    "  fig.tight_layout()\n",
    "  # Graph 1\n",
    "  axs[0, 0].pie(x = split_wise_numbers, explode=split_wise_explode, labels=split_wise_labels, autopct='%1.1f%%', startangle=90)\n",
    "  axs[0, 0].axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "  axs[0, 0].set_title ('Train, Test, Validation Split')\n",
    "  axs[0, 1].pie(x = total_label_wise_split, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "  axs[0, 1].axis('equal')\n",
    "  axs[0, 1].set_title ('Label wise split for entire Dataset')\n",
    "\n",
    "  # Graph 3\n",
    "  X = np.arange(3)\n",
    "  axs[0, 2].bar(X + 0.00, split_wise_label_wise[0], color = 'b', width = 0.25)\n",
    "  axs[0, 2].bar(X + 0.25, split_wise_label_wise[1], color = 'g', width = 0.25)\n",
    "  axs[0, 2].bar(X + 0.50, split_wise_label_wise[2], color = 'r', width = 0.25)\n",
    "  axs[0, 2].legend(labels=labels)\n",
    "  axs[0, 2].set_xticklabels([\"\", split_wise_labels[0], \"\", split_wise_labels[1], \"\", split_wise_labels[2]]) \n",
    "  axs[0, 2].set_title ('Split Wise, Label Wise Data')\n",
    "\n",
    "  axs[1, 0].hist(sen_length['train']['premise'], bins=50, label=\"premise length in train set\", alpha=0.5)\n",
    "  axs[1, 0].hist(sen_length['validation']['premise'], bins=50, label=\"premise length in validation set\", alpha=0.5)\n",
    "  axs[1, 0].legend(loc='best')\n",
    "  axs[1, 0].set_title ('Premise Length Comparison')\n",
    "  axs[1, 0].set_xlabel('Sentence Length')\n",
    "  axs[1, 0].set_ylabel('Frequency')\n",
    "  \n",
    "  axs[1, 1].hist(sen_length['train']['hypothesis'], bins=50, label=\"hypothesis length in train set\", alpha=0.5)\n",
    "  axs[1, 1].hist(sen_length['validation']['hypothesis'], bins=50, label=\"hypothesis length in validation set\", alpha=0.5)\n",
    "  axs[1, 1].legend(loc='best')\n",
    "  axs[1, 1].set_title ('Hypothesis Length Comparison')\n",
    "  axs[1, 1].set_xlabel('Sentence Length')\n",
    "  axs[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "  axs[1, 2].hist(sen_length['train']['sum'], bins=50, label=\"Input length in train set\", alpha=0.5)\n",
    "  axs[1, 2].hist(sen_length['validation']['sum'], bins=50, label=\"Input length in validation set\", alpha=0.5)\n",
    "  axs[1, 2].legend(loc='best')\n",
    "  axs[1, 2].set_title ('Input Length Comparison')\n",
    "  axs[1, 2].set_xlabel('Sentence Length')\n",
    "  axs[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "  print (\"Cannot plot the graphs: \", str (e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset (dataset, model_checkpoint):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "  \n",
    "  def tokenize_function (examples):\n",
    "    return tokenizer (examples['premise'], \n",
    "                      examples['hypothesis'],\n",
    "                      add_special_tokens = True,\n",
    "                      max_length=64, \n",
    "                      padding = 'max_length', \n",
    "                      truncation = True)\n",
    "    \n",
    "  tokenized_datasets = dataset.map (tokenize_function,\n",
    "                                  batched = True,\n",
    "                                  remove_columns = ['premise', 'hypothesis']\n",
    "                                  ).with_format(\"torch\")\n",
    "  return (tokenizer, tokenized_datasets)\n",
    "\n",
    "def classification_model (dataset, model_checkpoint):\n",
    "\n",
    "\n",
    "\n",
    "  model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, trust_remote_code = True,\n",
    "                                                            problem_type=\"single_label_classification\",\n",
    "                                                             num_labels = 3,#len (classes), \n",
    "                                                             id2label = int2label, \n",
    "                                                             label2id = label2int\n",
    "                                                             )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tokenizer, tokenized_dataset) = tokenize_dataset (dataset, model_checkpoint = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = classification_model(dataset=dataset, model_checkpoint = checkpoint)\n",
    "#for the pre-trained models in this thesis\n",
    "# model2.bert.embeddings = model.base_model.embeddings\n",
    "# model2.bert.encoder = model.base_model.encoder\n",
    "# model2.bert.pooler.dense = torch.nn.Linear(in_features=768, out_features=768, bias=True)\n",
    "# model2.bert.pooler.activation = torch.nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one forward pass to check if the code is working correctly\n",
    "outputs = model2(input_ids=tokenized_dataset['train']['input_ids'][0].unsqueeze(0), labels = tokenized_dataset['train'][0]['label'].unsqueeze(0))\n",
    "outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd \n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "   \n",
    "    probs = torch.softmax(torch.Tensor(predictions), dim=1)\n",
    "    y_pred = torch.argmax(probs, dim=1)\n",
    "    y_true = labels\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, y_pred, average=\"macro\")\n",
    "    f1_w = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    acc = accuracy_score(labels, y_pred)\n",
    "    #cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    #print('Confusion Matrix\\n')\n",
    "    #print(cf_matrix)\n",
    "    \n",
    "    # df_cm = pd.DataFrame((cf_matrix/np.sum(cf_matrix)) *10, index = [i for i in g_labels.names],\n",
    "    #                   columns = [i for i in g_labels.names])\n",
    "    # plt.figure(figsize = (12,7))\n",
    "    # sn.heatmap(df_cm, annot=True)\n",
    "    \n",
    "    return {\"accuracy\": acc,\n",
    "            \"f1\": f1, \n",
    "            \"f1_weighted\" : f1_w,\n",
    "            \"precision\": precision, \n",
    "            \"recall\": recall\n",
    "            }\n",
    "    \n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "metric_name = \"accuracy\"\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"training_with_callbacks\",\n",
    "    #evaluation_strategy = 'epoch', #IntervalStrategy.STEPS,\n",
    "    #save_strategy = \"epoch\",\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate= 1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=1e-4,\n",
    "    max_grad_norm= 5.0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    "    seed = 9000,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model2,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset['validation'], \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pred = trainer.predict(tokenized_dataset[\"test\"])\n",
    "raw_pred.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
