{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel\n",
    "import CXRBERT\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from eval import evaluate, plot_roc, accuracy, sigmoid, bootstrap, compute_cis\n",
    "from typing import Tuple\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files={ \"test\": \"full_test_with_c.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in dataset['test'].features.keys() if label not in ['subject_id', 'study_id', 'report', 'c_report']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cxr_pair_template = (\"no {}\", \"has {}\")\n",
    "cxr_pair_template = (\"findings suggesting {}\", \"no evidence of {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"allenai/scibert_scivocab_cased\"\n",
    "config = CXRBERT.CXRBertConfig.from_pretrained(checkpoint)\n",
    "tokenizer = CXRBERT.CXRBertTokenizer.from_pretrained(checkpoint,padding=\"max_length\", truncation=True, max_length=512)\n",
    "model = CXRBERT.CXRBertModel(config).from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './CXR-BERT/1qbqcmfd/checkpoints/epoch=49-step=25000.ckpt'\n",
    "model_cpt = torch.load(path)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in model_cpt['state_dict'].items():\n",
    "    name = k[6:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval() #model needs to be in evaluation state\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"c_report\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['test'].column_names)\n",
    "encoded_dataset.set_format(\"torch\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_zeroshot_classifier(classnames, templates, model, context_length=77):\n",
    "    #similar to CLIP zeroshot\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        # compute embedding through model for each class\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname) for template in templates] # format with class\n",
    "            texts = tokenizer(texts, padding = \"max_length\", max_length=20) # tokenize\n",
    "            \n",
    "            class_embeddings_out = model(torch.tensor(texts['input_ids']).to(device), torch.Tensor(texts['attention_mask']).to(device)) # embed with text encoder\n",
    "            \n",
    "            class_embeddings = class_embeddings_out['last_hidden_state'][:,0,:]\n",
    "            #\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "          \n",
    "            # average over templates if using more than 1 template at the same time\n",
    "            #class_embedding = class_embeddings.mean(dim=0) \n",
    "            # norm over new averaged templates\n",
    "            #class_embedding /= class_embedding.norm() \n",
    "            \n",
    "            zeroshot_weights.append(class_embeddings)\n",
    "            \n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=0)\n",
    "    return zeroshot_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "def predict(loader, model, zeroshot_weights, softmax_eval=True, verbose=0): \n",
    "    \"\"\"\n",
    "    FUNCTION: predict\n",
    "    ---------------------------------\n",
    "    This function runs the cxr images through the model \n",
    "    and computes the cosine similarities between the images\n",
    "    and the text embeddings. \n",
    "    \n",
    "    args: \n",
    "        * loader -  PyTorch data loader, loads in cxr images\n",
    "        * model - PyTorch model, trained clip model \n",
    "        * zeroshot_weights - PyTorch Tensor, outputs of text encoder for labels\n",
    "        * softmax_eval (optional) - Use +/- softmax method for evaluation \n",
    "        * verbose (optional) - bool, If True, will print out intermediate tensor values for debugging.\n",
    "        \n",
    "    Returns numpy array, predictions on all test data samples. \n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(loader)):\n",
    "            input_ids = data['input_ids'].to(device)\n",
    "            attention_mask = data['attention_mask'].to(device)\n",
    "            # predict\n",
    "            text_features = model(input_ids, attention_mask)[0]\n",
    "            text_features = text_features[:,0,:]\n",
    "\n",
    "            # obtain logits\n",
    "            y_pred_arr = []\n",
    "            # obtain logits\n",
    "            for class_weight in zeroshot_weights:\n",
    "                logits = text_features @ class_weight.T # (1, 2)\n",
    "                logits = logits.cpu().numpy()\n",
    "                \n",
    "                \n",
    "                sigmoid = torch.nn.Sigmoid()\n",
    "                norm_logits = normalize(logits, axis=1, norm='l1')  #no need, already normalized?\n",
    "                norm_logits = sigmoid(norm_logits)\n",
    "                y_pred_arr.append(norm_logits[0][0])\n",
    "            \n",
    "            \n",
    "         \n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = my_zeroshot_classifier(labels, cxr_pair_template,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(encoded_dataset['test'], batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred, test_probs = predict(test_dataloader, model,zeroshot_weights)\n",
    "test_y_true = encoded_dataset['test']['labels'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "val_best_thresholds = pd.read_csv('./validation/thresholds/' + path + '.csv')\n",
    "val_best_thresholds = np.array(val_best_thresholds['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_recall_fscore_support,precision_recall_curve\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "import evaluate\n",
    "probs = sigmoid(torch.Tensor(test_probs))\n",
    "#y_pred = np.zeros(probs.shape)\n",
    "\n",
    "pred_labels = np.zeros_like(probs)\n",
    "for i in range(test_probs.shape[1]):\n",
    "    pred_labels[:, i] = np.where(test_probs[:, i] > val_best_thresholds[i], 1, 0)\n",
    "\n",
    "y_pred = pred_labels\n",
    "y_true = test_y_true\n",
    "\n",
    "f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "roc_auc = roc_auc_score(y_true, probs, average = 'micro')\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')\n",
    "#roc_auc_mac = roc_auc_score(y_true, y_pred, average = 'macro')\n",
    "roc_auc_mac = roc_auc_score(y_true, probs, average = 'macro')\n",
    "f1_w_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "roc_auc_w = roc_auc_score(y_true, probs, average = 'weighted')\n",
    "#############################################################################\n",
    "roc_auc_score2 = evaluate.load(\"roc_auc\", \"multilabel\")\n",
    "results = roc_auc_score2.compute(references=y_true, prediction_scores=probs, average = None)['roc_auc']\n",
    "\n",
    "# return as dictionary\n",
    "metrics = {'f1_micro': f1_micro_average,\n",
    "        'roc_auc_micro': roc_auc,\n",
    "        'f1_macro': f1_macro_average,\n",
    "        'roc_auc_macro': roc_auc_mac,\n",
    "        'f1_weighted': f1_w_average,\n",
    "        'roc_auc_weighted': roc_auc_w,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc_per_class': [round(res, 3) for res in results]\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
